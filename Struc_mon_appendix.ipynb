{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Struc-mon_appendix.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SZ0VtYnMfSHE",
        "yY0kq2JEfiyY",
        "E6bAIJgD0YJb",
        "wdciiDys62NI",
        "jeGYOe-M_PEN",
        "Uxldwa9-AFtO",
        "XX9IWTuQPQc_",
        "mOKf4-vY6P0S",
        "A9pAF29YoQVE"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesperiksson/struc-mon/blob/master/Struc_mon_appendix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l11DflmDq5f",
        "colab_type": "text"
      },
      "source": [
        "# Data source\n",
        "Specify the source of the data to be used, the code below will mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjfjyLk_D2hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzvXTba9x-fp",
        "colab_type": "text"
      },
      "source": [
        "# Settings\n",
        "There are mainly three cells which control the program\n",
        "## Main Settings - make adjustments here\n",
        "The `use` variable decides weather to use a MLP-model or a LSTM-model.\n",
        "The `name` variable sets the name of the variable which will be used when saving models, architectures and figures.\n",
        "The `retrain` variable tells the program weather it should initiate a new training process even though there is a trained model with that name.\n",
        "The `overwrite` variable tells the program weather it should load the same `architecture` dictionary as was used when the model was created. If any changes are to be made this must be set to `True`.\n",
        "If `architecture_mode` is set to `2` all the settings above can be overrided for each model that is used.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDvVRAZJ8c-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use = 'MLP'\n",
        "name = 'placeholder'\n",
        "retrain = False\n",
        "overwrite = True\n",
        "train_process = 'entire_set'\n",
        "'''\n",
        "'entire_set' - fit all data into X and Y\n",
        "'batch by batch' - fit a series into X and Y, train, fit next series into X and Y, train, repeat\n",
        "'generator' - keep generating sample after sample, working through all available data\n",
        "'''\n",
        "architecture_mode = 2\n",
        "'''\n",
        "mode = 1 : Uses the architecture specified only\n",
        "mode = 2 : Uses several architectures specified below\n",
        "'''\n",
        "eval_mode = 1\n",
        "'''\n",
        "mode = 1: evaluation()\n",
        "mode = 2: batch_evaluation()\n",
        "'''\n",
        "dataset = 4\n",
        "'''\n",
        "dataset 1 - Random speeds\n",
        "dataset 2 - Same speeds for all damage cases plus extra speeds for healthy\n",
        "dataset 3 - dataset 2 with extra healthy\n",
        "dataset 4 - dataset 1 with extra healthy\n",
        "'''\n",
        "save_plots = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrzwDZdyLye",
        "colab_type": "text"
      },
      "source": [
        "This function handles the implications of the `retrain` setting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AHhKuaDYUMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup(use, name, retrain, overwrite, base_model = None):\n",
        "    setup_dict = {\n",
        "        'use' : use,\n",
        "        'name' : name,\n",
        "        'dataset' : dataset, # 1 # our_measurements, 2: our_measurements3\n",
        "        'retrain' : retrain, # Loads the model to train it further, the original model is untouched\n",
        "        'appendix' : 'v2',\n",
        "        'overwrite' : overwrite\n",
        "    }\n",
        "    if setup_dict['retrain'] == True:\n",
        "        setup_dict.update({\n",
        "            'base_model' : base_model, # if retrain is True, this is the model to be loaded\n",
        "        })\n",
        "        setup_dict.update({\n",
        "            'fname' : setup_dict['use'] + setup_dict['name'],\n",
        "            'fname_base' : setup_dict['use'] + setup_dict['base_model']  \n",
        "        })\n",
        "        setup_dict.update({\n",
        "            'fpath' : '/content/drive/My Drive/KTH/Examensarbete/struc-mon (kopia)/models/' + setup_dict['fname'] + '.pkl',\n",
        "            'fpath_base' : '/content/drive/My Drive/KTH/Examensarbete/struc-mon (kopia)/models/' + setup_dict['fname_base'] + '.pkl'  \n",
        "        })\n",
        "    elif setup_dict['retrain'] == False:\n",
        "        setup_dict.update({\n",
        "            'fname' : setup_dict['use'] + setup_dict['name'],\n",
        "            'fname_base' : setup_dict['use'] + setup_dict['name']\n",
        "        })\n",
        "        setup_dict.update({\n",
        "            'fpath' : '/content/drive/My Drive/KTH/Examensarbete/struc-mon (kopia)/models/' + setup_dict['fname'] + '.pkl',\n",
        "            'fpath_base' : '/content/drive/My Drive/KTH/Examensarbete/struc-mon (kopia)/models/' + setup_dict['fname'] + '.pkl',  \n",
        "        })\n",
        "        \n",
        "    return setup_dict\n",
        "#setup_dict = setup('LSTM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnaVx4-myTKo",
        "colab_type": "text"
      },
      "source": [
        "### Import function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlM9w1F5YpMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "def load_architecture(setup_dict):\n",
        "    with open(setup_dict['fpath'], 'rb') as f:\n",
        "        return pickle.load(f) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS7p5HqoyV4-",
        "colab_type": "text"
      },
      "source": [
        "### Save function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWl81rzMamVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_architecture(arch):\n",
        "    with open(arch['model_path']+ arch['fname'] +'.pkl', 'wb') as f:\n",
        "        pickle.dump(arch, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4k2Y-7wyZec",
        "colab_type": "text"
      },
      "source": [
        "Use this function to inspect an already existing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHIu70GjS2_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def inspect(name, use, path):\n",
        "    d = {'fpath' : path+use+name+'.pkl'}\n",
        "    arch = load_architecture(d)\n",
        "    return arch\n",
        "inspect(name = '', use = 'LSTM')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFbLMmJyyQQx",
        "colab_type": "text"
      },
      "source": [
        "## Architecture\n",
        "The `architecture` dictionary is where the program is controlled from. All the settings are made here.\n",
        "Some entries that need further explanation:\n",
        "\n",
        "`'model'`- Specifies which `Keras.model` object to use. There are a bunch already implemented.\n",
        "    How to implement a new function:\n",
        "        1. Add a cell below \"Models\" and create a function that makes the desired model in the same way as the already implemented functions\n",
        "        2. Add the function to the `model_dict` in the `__init__()` function of the `Ç¸euralNet` class.\n",
        "        3. Chose the respective keys as the value for the `model` key in `architecture`\n",
        "\n",
        "`preprocess_type` - Specifies which type of preprocessing to apply on the data. \n",
        "    `data` - No preprocess is made, all timesteps are kept\n",
        "    `peaks` - Only peak values and their timesteps are kept\n",
        "    \n",
        "`Normalization` - Specifies which normalization to apply on the data.\n",
        "    `Maximum` - All accelerations are subtracted by the mean and divided by the maximum.\n",
        "    `L-1` - All accelerations are subtracted by the mean and divided by the L-1 norm. \n",
        "    `L-2` - All accelerations are subtracted by the mean and divided by the L-2 norm.\n",
        "    `Raw data` - No normalization is made\n",
        "    \n",
        "`loss`, `metric`, `val_loss` - Specifies the loss function and any further metrics. See https://keras.io/api/metrics/ for more info. \n",
        "\n",
        "`shuffle` - Weahter to shuffle the samples in a batch during the training process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEyqIBdbYqW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_arch(setup_dict, changes = {}):\n",
        "    architecture = None \n",
        "    if setup_dict['overwrite'] == False:\n",
        "        try:\n",
        "            architecture = load_architecture(setup_dict)\n",
        "            print('Loaded architecture: ', setup_dict['fname'])\n",
        "            new_model = False\n",
        "        except IOError: \n",
        "            new_model = True\n",
        "    elif setup_dict['overwrite'] == True:    \n",
        "        new_model = True\n",
        "    if new_model == True:\n",
        "        architecture = setup_dict\n",
        "        architecture.update({\n",
        "            'active_sensors' : ['90'],\n",
        "            'predict' : 'accelerations', # accelerations or damage\n",
        "            # Paths\n",
        "            'data_path'       : , # Enter \n",
        "            'model_path'      : , # the\n",
        "            'plot_path'       : , # paths\n",
        "            'series_plot_path': , # here\n",
        "            'model_plot_path' : , #\n",
        "            'loss_path'       : , #\n",
        "            'prediction_path' :   #\n",
        "            })\n",
        "        sensor_dict = {}\n",
        "        for i in range(len(architecture['active_sensors'])):\n",
        "            sensor_dict.update({\n",
        "                architecture['active_sensors'][i] : i\n",
        "                })\n",
        "        architecture.update({\n",
        "            'sensors' : sensor_dict\n",
        "            })\n",
        "        architecture.update({\n",
        "            'model' : 'Single layer MLP',\n",
        "            # Net configuaration\n",
        "            'n_units' : {\n",
        "                'first' : 30, \n",
        "                'second' : 15, \n",
        "                'third' : 10, \n",
        "                'fourth' : 5\n",
        "                },\n",
        "            'bias' : True,\n",
        "            'n_pattern_steps' : 15, # Kan Ã¤ndras\n",
        "            'n_target_steps' : 1,\n",
        "            'pattern_delta' : 1,\n",
        "            # Sensor parameters\n",
        "            'pattern_sensors' : ['90'],\n",
        "            'target_sensor' : '90',\n",
        "            'target_sensors' : ['90'],\n",
        "            # Training parameters\n",
        "            'batch_size' : 16,\n",
        "            'positions' : False,\n",
        "            'data_split' : {\n",
        "                'train':30, \n",
        "                'validation':20, \n",
        "                'test':50\n",
        "                }, # sorting of data \n",
        "            'delta' : 1, # Kan Ã¤ndras\n",
        "            'Dense_activation' : 'tanh',\n",
        "            'early_stopping' : True,\n",
        "            'epochs' : 200,\n",
        "            'learning_rate' : 0.001, # 0.001 by default\n",
        "            'min_delta' : 0.0001,\n",
        "            'preprocess_type' : 'data',\n",
        "            'patience' : 3,\n",
        "            'shuffle' : True,\n",
        "            'steps_per_epoch' : 100,\n",
        "            'validation_steps' : 50,\n",
        "            'dropout_rate' : 0.2, # Only in use if model utilizes dropout\n",
        "            # Loss function\n",
        "            'loss' : 'mse',\n",
        "            'metric': 'rmse',\n",
        "            'val_loss' : 'val_loss',\n",
        "            # Data\n",
        "            'normalization' : 'L-2',\n",
        "            'speed_unit' : 'km/h',\n",
        "            'seed' : 3,\n",
        "            'mode' : '2', # 1 - , 2- \n",
        "            'from' : 0,\n",
        "            'to' : -1,\n",
        "            'random_mode' : 'debug', # test or debug\n",
        "            # Noise\n",
        "            'noise' : False,\n",
        "            'noise_mean' : 0,\n",
        "            'noise_var' : 0.001,\n",
        "            # Evaluation parameters\n",
        "            'eval_batch_size' : 32,\n",
        "            # Model saving\n",
        "            'save_periodically' : False,\n",
        "            'save_interval' : 10, # Number of series to train on before saving\n",
        "            'foi' : 'normalization', # feature of interest, the one to be printed in the title of plots\n",
        "            # Classification\n",
        "            'probability_limit' : 0.90, # phi-value for CDF-classificaton\n",
        "            'fitting_points' : 20, # Number of sample results to use to fit the regression line\n",
        "            'z-score' : 2.2, # Distance (number of standard deviations) to the decision boundary.\n",
        "            'confusion_matrices' : {\n",
        "                'cdf' : 'CDF',\n",
        "                'poly' : 'Polynomial approximation'\n",
        "                },\n",
        "            'poly_deg' : 1\n",
        "        })\n",
        "        architecture.update({\n",
        "            'features' : len(architecture['pattern_sensors']) + architecture['positions']\n",
        "        })\n",
        "        architecture.update(changes)\n",
        "        save_architecture(architecture)\n",
        "    return architecture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9-WBN4g3WuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYUCnvd7MYxr",
        "colab_type": "text"
      },
      "source": [
        "# Mode 2\n",
        "if `architecture_mode` is set to 2, i.e. several models are loaded at once the piece of code below is executed. In order to add models, add entries in the `architectures` dictionary. Here it is set up to compare 8 different models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuzD1AKN6MC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "architecture = get_arch(setup(use,name,retrain,overwrite))\n",
        "if architecture_mode == 2:\n",
        "    rt = False\n",
        "    ow = True\n",
        "    name1 = '' # enter name\n",
        "    name2 = '' # enter name\n",
        "    architectures = {\n",
        "        '1' : get_arch(setup(use = 'MLP', name = name1+'1_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer MLP', 'normalization' : 'Raw data'}),\n",
        "        '2' : get_arch(setup(use = 'MLP', name = name1+'2_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer MLP', 'normalization' : 'Maximum'}),\n",
        "        '3' : get_arch(setup(use = 'MLP', name = name1+'3_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer MLP', 'normalization' : 'L-1'}),\n",
        "        '4' : get_arch(setup(use = 'MLP', name = name1+'4_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer MLP', 'normalization' : 'L-2'}),\n",
        "        '5' : get_arch(setup(use = 'LSTM', name = name2+'1_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer LSTM', 'normalization' : 'Raw data'}),\n",
        "        '6' : get_arch(setup(use = 'LSTM', name = name2+'2_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer LSTM', 'normalization' : 'Maximum'}),\n",
        "        '7' : get_arch(setup(use = 'LSTM', name = name2+'3_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer LSTM', 'normalization' : 'L-1'}),\n",
        "        '8' : get_arch(setup(use = 'LSTM', name = name2+'4_'+str(dataset), retrain = rt, overwrite = ow), changes = {'model' : 'Single layer LSTM', 'normalization' : 'L-2'})\n",
        "    }\n",
        "    arch_keys = list(architectures.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ0VtYnMfSHE",
        "colab_type": "text"
      },
      "source": [
        "# Import function\n",
        "This function creates instances of the `DataBatch` class, where each batch contains one data series, a.k.a one passage. There are subclasses `data`, `peaks`, `extrema` and `frequency`. Data contains the entire series, peaks contains all peak accelerations, extrema contains all local and global extreme values and frquency contains a frequency spectrum. \n",
        "\n",
        "If this program is to be used with some otther data this function will have to be rewritten in order to be able to fit that data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJV36TCRfcc5",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import h5py\n",
        "import scipy as sp\n",
        "import scipy.signal\n",
        "def fit_to_NN(\n",
        "        data_split, \n",
        "        path, \n",
        "        healthy_percentage, \n",
        "        arch):\n",
        "    \n",
        "    types = arch['preprocess_type']\n",
        "    paths = {}\n",
        "\n",
        "    for i in range(len(arch['active_sensors'])):\n",
        "        paths.update(\n",
        "            {arch['active_sensors'][i] : path+'s'+arch['active_sensors'][i]+'/'})\n",
        "    if arch['random_mode'] == 'debug':\n",
        "        seed = arch['seed']\n",
        "    elif arch['random_mode'] == 'test':\n",
        "        seed = random.randint(0,10000)\n",
        "    file_list = os.listdir(paths[arch['active_sensors'][0]])\n",
        "    file_list.sort()\n",
        "    random.Random(seed).shuffle(file_list)\n",
        "\n",
        "    speeds = np.empty([len(file_list)])\n",
        "    for i in range(len(file_list)):\n",
        "        if len(file_list[i]) == 9:\n",
        "            speeds[i] = int(file_list[i][0:5])\n",
        "        elif len(file_list[i]) == 10:\n",
        "            speeds[i] = int(file_list[i][0:6])\n",
        "        else: \n",
        "            print('error')\n",
        "    normalized_speeds = (speeds-min(speeds))/(max(speeds)-min(speeds))\n",
        "\n",
        "    n_files = len(file_list)\n",
        "    data_stack = {}\n",
        "    preprocess_stack = {}\n",
        "    peaks_stack = {}\n",
        "    frequency_stack = {}\n",
        "    extrema_stack = {}\n",
        "    start = arch['from']\n",
        "    to = arch['to']\n",
        "    diff = to-start\n",
        "    for i in range(n_files):\n",
        "        data = [None]*len(paths)\n",
        "        for j in range(len(paths)):\n",
        "            mat = h5py.File(paths[arch['active_sensors'][j]] + file_list[i], 'r')\n",
        "            data[j] = mat.get('acc')[1,start:to]\n",
        "\n",
        "        if i/n_files < (data_split['train']/100):\n",
        "            category = 'train'\n",
        "        elif i/n_files > (data_split['validation']/100) and i/n_files < ((data_split['train']+data_split['validation'])/100):\n",
        "            category = 'validation'\n",
        "        else:\n",
        "            category = 'test'\n",
        "        if 'data' in types:\n",
        "            data_stack.update({\n",
        "                'batch'+str(i) : DataBatch(\n",
        "                    arch,\n",
        "                    data,            \n",
        "                    i,\n",
        "                    speeds[i]/1000,\n",
        "                    normalized_speeds[i],\n",
        "                    category,\n",
        "                    healthy_percentage)\n",
        "                    })\n",
        "        if 'frequency' in types:\n",
        "            frequency_stack.update({\n",
        "                'batch'+str(i) : frequencySpectrum(\n",
        "                    arch,\n",
        "                    data,\n",
        "                    i,\n",
        "                    speeds[i]/1000,\n",
        "                    normalized_speeds[i],\n",
        "                    category,\n",
        "                    healthy_percentage)\n",
        "                    })\n",
        "        if 'peaks' in types:\n",
        "            peaks_stack.update({\n",
        "                'batch'+str(i) : peaks(\n",
        "                    arch,\n",
        "                    data,\n",
        "                    i,\n",
        "                    speeds[i]/1000,\n",
        "                    normalized_speeds[i],\n",
        "                    category,\n",
        "                    healthy_percentage)\n",
        "                    })\n",
        "\n",
        "    preprocess_stack.update({\n",
        "        'data' : data_stack,\n",
        "        'frequency' : frequency_stack,\n",
        "        'peaks' : peaks_stack,\n",
        "        'extrema' : extrema_stack\n",
        "        })    \n",
        "\n",
        "    return preprocess_stack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY0kq2JEfiyY",
        "colab_type": "text"
      },
      "source": [
        "# `DataBatch` base class \n",
        "`__init__` function creates instance with all the useful attributes. \n",
        "The `data_dict` allow us to pick the type of normalization from `architecture`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNCcgbskDpfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "class DataBatch():\n",
        "    def __init__(self, a, data, batch_num, speed, normalized_speed, category, damage_state):\n",
        "        self.data = np.array(data)\n",
        "        self.raw_data = np.array(data)\n",
        "        self.l1 = preprocessing.normalize(\n",
        "            X = self.raw_data,\n",
        "            norm = 'l1')\n",
        "        self.l2 = preprocessing.normalize(\n",
        "            X = self.raw_data,\n",
        "            norm = 'l2')\n",
        "        self.max = preprocessing.normalize(\n",
        "            X = self.raw_data,\n",
        "            norm = 'max')\n",
        "        self.sensors = np.shape(data)[0]\n",
        "        self.batch_num = batch_num\n",
        "        self.category = category\n",
        "        self.n_steps = np.shape(self.data)[1]\n",
        "        speed_dict = {\n",
        "            'km/h' : speed, \n",
        "            'm/s' : (speed*3.6/10)\n",
        "            }\n",
        "        self.speed = speed_dict[a['speed_unit']]\n",
        "        self.normalized_speed = normalized_speed\n",
        "        self.damage_state = damage_state\n",
        "        self.normalized_damage_state = damage_state/100\n",
        "        self.timestep = 0.001\n",
        "        self.timesteps = np.arange(0, self.n_steps, 1)\n",
        "        self.steps = [None]*self.sensors\n",
        "        self.indices = [None]*self.sensors\n",
        "        self.delta = [None]*self.sensors\n",
        "        for i in range(self.sensors):\n",
        "            self.steps[i] = self.n_steps\n",
        "        self.data_dict = {\n",
        "            'Raw data'  : self.raw_data,    # Unaltered\n",
        "            'L-1'       : self.l1,          # L1-norm\n",
        "            'L-2'       : self.l2,          # L2-norm\n",
        "            'Maximum'   : self.max          # Normalized to the greatest acceleration\n",
        "        }\n",
        "        #self.data = self.data_dict[a['normalization']]\n",
        "        self.n_series = int(self.n_steps)-int(a['delta']*a['n_pattern_steps'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwWjl3_s55Cq",
        "colab_type": "text"
      },
      "source": [
        "## `Peaks`\n",
        "The aforementioned `peaks` class. The `self.timesteps` attribute contains the indices of the timesteps where the peak accelerations occur. The main use for this sub class would be for when there is no powerful hardware available, since it reduces the number of datapoints but retains most of the information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G0lLt-y55LN",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title \n",
        "class Peaks(DataBatch):\n",
        "    def __init__(self, data, batch_num, speed, normalized_speed, category, damage_state):  \n",
        "        super().__init__(data, batch_num, speed, normalized_speed, category, damage_state)\n",
        "        self.peaks = [None]*self.sensors\n",
        "        for i in range(self.sensors):\n",
        "            self.indices[i], properties = sp.signal.find_peaks(\n",
        "                self.data[i], \n",
        "                height = None, \n",
        "                threshold = None,\n",
        "                distance = 2,\n",
        "                prominence = None,\n",
        "                width = None)\n",
        "            self.peaks[i] = self.data[i][self.indices[i]]\n",
        "            \n",
        "            delta = np.diff(self.indices[i])\n",
        "            self.delta[i] = delta/max(delta)\n",
        "        self.n_steps = np.shape(self.peaks[0])[0] # overwrite data\n",
        "        self.timesteps = self.indices[0]\n",
        "        self.data = self.peaks "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuQCIqnHfl5t",
        "colab_type": "text"
      },
      "source": [
        "# Data splitting\n",
        "The two modes of dividing the available data into sets. Which one is used is decided by the `'mode'` parameter below training parameters in `architecture`. \n",
        "\n",
        "`data_split_mode1` takes the available healthy data assigned for validation and test according to the `'data_split'` parameter (below training parameters in architecture) and puts it aside. (Usually it is `{'train':60, 'validation':20, 'test':20}`).\n",
        "Furthermore, it sets all available unhealthy data as test data.\n",
        "\n",
        "`data_split_mode2` divides all available data, both healthy and unhealthy according to the `'data_split'` parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXje9RQoDpfD",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def data_split_mode1(a):\n",
        "    series_stack = {}\n",
        "    damage_dir_list = os.listdir(a['path'])\n",
        "    for j in range(len(damage_dir_list)):\n",
        "        series_stack.update({\n",
        "            damage_dir_list[j] : fit_to_NN(\n",
        "                a['data_split'],\n",
        "                a['data_path']+'/'+damage_dir_list[j]+'/', \n",
        "                int(damage_dir_list[j][:-1]),\n",
        "                a)\n",
        "            })\n",
        "    return series_stack\n",
        "\n",
        "def data_split_mode2(a):\n",
        "    series_stack = {}\n",
        "    damage_dir_list = os.listdir(a['data_path'])\n",
        "    for j in range(len(damage_dir_list)):\n",
        "        if damage_dir_list[j] == '100%':\n",
        "            series_stack.update({\n",
        "                damage_dir_list[j] : fit_to_NN(\n",
        "                    a['data_split'],\n",
        "                    a['data_path']+'/'+damage_dir_list[j]+'/',\n",
        "                    int(damage_dir_list[j][:-1]),\n",
        "                    a)\n",
        "                })\n",
        "        else:\n",
        "            series_stack.update({\n",
        "                damage_dir_list[j] : fit_to_NN(\n",
        "                    {'train' : 0, 'validation' : 0, 'test' : 100},\n",
        "                    a['data_path']+'/'+damage_dir_list[j]+'/',\n",
        "                    int(damage_dir_list[j][:-1]),\n",
        "                    a)\n",
        "                })\n",
        "    return series_stack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrsfJWyt6pY2",
        "colab_type": "text"
      },
      "source": [
        "Piece of code which activates the chosen split function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qew8aVZ6phS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if architecture['mode'] == '1':\n",
        "    series_stack = data_split_mode1(architecture)\n",
        "    '''\n",
        "    This is the normal case where all available data is divided into train/ test/ validation\n",
        "    '''\n",
        "\n",
        "elif architecture['mode'] == '2':\n",
        "    series_stack = data_split_mode2(architecture)\n",
        "    '''\n",
        "    This is special case where only healthy data is used for training and \n",
        "    all damaged data is used for testing.\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijSFzqcWgO9V",
        "colab_type": "text"
      },
      "source": [
        "Let's find the length of the longest series\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV62fCppgPJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def find_longest(arch, stack):\n",
        "    biggest = 0\n",
        "    dam_keys = list(stack.keys())\n",
        "    for i in range(len(dam_keys)):\n",
        "        batch_keys = list(stack[dam_keys[i]][arch['preprocess_type']])\n",
        "        for j in range(len(batch_keys)):\n",
        "            #print(batch_keys)\n",
        "            length = max(np.shape(stack[dam_keys[i]][arch['preprocess_type']][batch_keys[j]].data))\n",
        "            speed = stack[dam_keys[i]][arch['preprocess_type']][batch_keys[j]].speed\n",
        "            if length > biggest:\n",
        "                biggest = length\n",
        "                v = speed\n",
        "    return biggest, v\n",
        "biggest, v = find_longest(architecture, series_stack)\n",
        "print('Longest series: ', str(biggest), ' at the speed: ', v,' km/h')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkf5uJNa_t_Q",
        "colab_type": "text"
      },
      "source": [
        "# Data Inspection\n",
        "Have a look at the data we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKQJacTU_uGy",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_batch(stack, arch, plot_sensor = '90'): \n",
        "    stack = stack[arch['preprocess_type']]\n",
        "    side = min(int(np.floor(np.sqrt(len(stack)))),7)\n",
        "    rows = 2\n",
        "    cols = 2\n",
        "    plt.figure(\n",
        "        num = None, \n",
        "        figsize=(32, 24), \n",
        "        dpi = 80, \n",
        "        facecolor = 'w', \n",
        "        edgecolor = 'k'\n",
        "        )\n",
        "    fig, axs = plt.subplots(rows, cols, constrained_layout=True)\n",
        "    k = 0\n",
        "    print(axs)\n",
        "    for i in range(rows):            \n",
        "        for j in range(cols):        \n",
        "            series = stack['batch'+str(k)]\n",
        "            axs[i][j].plot(series.timesteps, series.data[arch['sensors'][plot_sensor],:], 'b', linewidth=0.1)\n",
        "            #plt.plot(stack[key].peaks_indices[sensor], stack[key].peaks[sensor], 'ro', linewidth = 0.4)\n",
        "            axs[i][j].set_title(str(series.speed)+'km/h')\n",
        "            #plt.set_xlabel('timesteps')\n",
        "            #plt.set_ylabel('acceleration')\n",
        "            k += 1\n",
        "        k += 1\n",
        "    plt.suptitle(str(series.damage_state)+'% Healthy at mid-span, registered at sensor: '+plot_sensor)\n",
        "    name = 'E'+str(series.damage_state)+'_d90_s'+plot_sensor+'.png'\n",
        "    #print(name)\n",
        "    #plt.savefig(name)\n",
        "    plt.show()\n",
        "\n",
        "    return\n",
        "DataBatch.plot_batch = plot_batch\n",
        "#DataBatch.plot_batch(series_stack['100%'], architecture)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6bAIJgD0YJb",
        "colab_type": "text"
      },
      "source": [
        "## Batch Comparison\n",
        "The function `plot_batch_comparison()` works best with dataset 2. It plots two series whith different speed on top of each other. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNAgpd82k5xr",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def plot_batch_comparison(stack, arch, number, states = ('95%', '57%'), plot_sensor = '90', save = False, plot = False):\n",
        "    if plot == False and save == False:\n",
        "        return\n",
        "    plt.figure(\n",
        "        num = None, \n",
        "        figsize=(16, 12), \n",
        "        dpi = 80, \n",
        "        facecolor = 'w', \n",
        "        edgecolor = 'k'\n",
        "        )\n",
        "    stack1 = stack[states[0]][arch['preprocess_type']]\n",
        "    series1 = stack1['batch'+str(number)]\n",
        "    stack2 = stack[states[1]][arch['preprocess_type']]\n",
        "    series2 = stack2['batch'+str(number)]\n",
        "    speed = series1.speed\n",
        "    tstart = 500\n",
        "    tend = 2500\n",
        "    #plt.plot(series1.timesteps,series1.data[0], linewidth = 0.4)\n",
        "    plt.plot(series1.timesteps[tstart:tend],series1.raw_data[0][tstart:tend], linewidth = 0.4)\n",
        "    #plt.plot(series1.timesteps,series1.l1[0], linewidth = 0.4)\n",
        "    #plt.plot(series1.timesteps,series1.l2[0], linewidth = 0.4)\n",
        "    #plt.plot(series1.timesteps,series1.max[0], linewidth = 0.4)\n",
        "    plt.plot(series2.timesteps[tstart:tend],series2.raw_data[0][tstart:tend], linewidth = 0.4)\n",
        "    #plt.legend([states[0],states[1]])\n",
        "    #plt.legend(['raw data', 'l1', 'l2', 'max'], markerscale = 10.0)\n",
        "    #plt.title('Comparison of normalization schemes')\n",
        "    plt.legend([state1,state2])\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Accelerations')\n",
        "    plt.text(\n",
        "        x = tstart, \n",
        "        y = 0, \n",
        "        s = 'Speed = ' + str(speed),\n",
        "        fontsize = 20\n",
        "        )\n",
        "    if save == True:\n",
        "            plt.savefig(fname = arch['series_plot_path']+'dataset_'+str(dataset)+'_'+str(speed)+'_'+state1+'_vs_'+state2+'_'+str(tstart)+'_'+str(tend)+'.png')\n",
        "    if plot == True:\n",
        "      plt.show()\n",
        "    \n",
        "state1 = '95%'\n",
        "state2 = '57%'\n",
        "'''\n",
        "dataset 1 - 100, 90, 81, 71, 62, 52, 43, 33\n",
        "dataset 2 - 100, 95, 93, 90, 86, 81, 76, 71, 67, 62, 57 \n",
        "'''\n",
        "for i in range(len(series_stack[state1])):\n",
        "    plot_batch_comparison(\n",
        "        series_stack, \n",
        "        architecture, \n",
        "        i, \n",
        "        (state1,state2), \n",
        "        save = False,\n",
        "        plot = True\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdciiDys62NI",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n",
        "Some necessary modules and objects. It is important to use the tensorflow implementation af the Keras API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubCFOmQmDpfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.models import Sequential, Model, model_from_json\n",
        "from tensorflow.python.keras.layers import Input, Dense, LSTM, CuDNNLSTM, concatenate, Activation, Reshape, Bidirectional, Flatten, Dropout\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.python.keras import metrics, regularizers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.python.keras import backend\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "\n",
        "tf.random.set_seed(42)# tensorflow seed fixing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeGYOe-M_PEN",
        "colab_type": "text"
      },
      "source": [
        "# `NeuralNet` class\n",
        "Most importantly contains the `self.model` attribute. The `model_dict` dictionary variable must be updated once a new model architecture is added. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJy_ReQF_PK7",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet():\n",
        "    def __init__(\n",
        "        self,\n",
        "        arch,\n",
        "        existing_model):\n",
        "\n",
        "        self.arch = arch\n",
        "        self.name = arch['name']\n",
        "        self.target_sensor = self.arch['sensors'][self.arch['target_sensor']]\n",
        "        self.pattern_sensors = self.arch['sensors'][self.arch['pattern_sensors'][0]]\n",
        "        self.sensor_to_predict = arch['sensors'][arch['target_sensor']]\n",
        "        if arch['early_stopping'] == True:\n",
        "            self.early_stopping = [keras.callbacks.EarlyStopping(\n",
        "                monitor = arch['val_loss'],\n",
        "                min_delta = arch['min_delta'], \n",
        "                patience = arch['patience'],\n",
        "                verbose = 1,\n",
        "                mode = 'auto',\n",
        "                restore_best_weights = True)]\n",
        "\n",
        "        else:\n",
        "            self.early_stopping = None\n",
        "        self.existing_model = existing_model\n",
        "        self.n_sensors = len(arch['sensors'])    \n",
        "        model_dict = {\n",
        "            'Single layer LSTM-CPU' : set_up_model6(arch),\n",
        "            'Two layer CPU-LSTM' : set_up_model7(arch),\n",
        "            'Single layer LSTM-non-CPU' : set_up_model1(arch), \n",
        "            'Two layer LSTM-non-CPU' : set_up_model2(arch),\n",
        "            'Two layer LSTM CPU position' : set_up_model3(arch),\n",
        "            'Single layer LSTM' : set_up_model8(arch),\n",
        "            'Two layer LSTM' : set_up_model4(arch),\n",
        "            'Three layer LSTM' : set_up_model9(arch),\n",
        "            'Four layer LSTM' : set_up_model12(arch),\n",
        "            'Single layer MLP' : set_up_model10(arch),\n",
        "            'Two layer MLP' : set_up_model5(arch),\n",
        "            'Three layer MLP' : set_up_model13(arch),\n",
        "            'Four layer MLP' : set_up_model11(arch),\n",
        "            'Single layer MLP dropout' : set_up_model14(arch),\n",
        "            'Single layer LSTM dropout' : set_up_model15(arch)\n",
        "            }     \n",
        "        metric_dict = {\n",
        "            'rmse' : [rmse],\n",
        "            'mse' : 'mse',\n",
        "            'val_loss' : 'val_loss',\n",
        "            'mae' : 'mae'\n",
        "            }\n",
        "        if self.existing_model == False:\n",
        "            model = model_dict[arch['model']]\n",
        "\n",
        "        elif self.existing_model == True:\n",
        "            model = load_model(arch)\n",
        "        else:\n",
        "            raise Error\n",
        "        optimizer = keras.optimizers.Adam(\n",
        "            learning_rate = arch['learning_rate'],\n",
        "            beta_1 = 0.9,\n",
        "            beta_2 = 0.999,\n",
        "            epsilon = 1e-07,\n",
        "            amsgrad = False)\n",
        "        model.compile(\n",
        "            optimizer = optimizer, \n",
        "            loss = arch['loss'],\n",
        "            metrics = metric_dict[arch['metric']])\n",
        "        fig = plot_model(\n",
        "            model, \n",
        "            to_file = arch['model_plot_path'] + arch['model'] + '.png',\n",
        "            show_shapes = False,\n",
        "            #show_dtype = True,\n",
        "            show_layer_names = False)\n",
        "        model.summary()\n",
        "        self.model = model\n",
        "        #self.score = None\n",
        "        #self.loss = [None]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELUAjFg76lql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(arch):\n",
        "    model_path = arch['model_path']+arch['fname_base']+'.json'\n",
        "    weights_path = arch['model_path']+arch['fname_base']+'.h5'\n",
        "    json_file = open(model_path)\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    loaded_model.load_weights(weights_path)\n",
        "    model = loaded_model\n",
        "    print('\\n Loaded model: ', arch['fname'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxldwa9-AFtO",
        "colab_type": "text"
      },
      "source": [
        "# Training methods\n",
        "The training method for the `NeuralNet` class. As arguments it requires firstly a `NeuralNet` object and secondly a dictionary of training series. The dictionary shall contain keys which indicate the damage state and values which, in turn, are also dictionaries. These dictionaries contains key-value pairs where the different data manipulations (i.e. data, peaks, extrema and frequency) are keys and the corresponding series stacks are the values. Finally, the series stacks are dictionaries where all the `DataBatch` objects have keys `batch0`, `batch1`, ... and so forth.\n",
        "I know this is a bit convoluted but it is what allows the entire program to be maniuplated via the `architecture` variable. \n",
        "\n",
        "The training process uses one series at a time to train. It is implemented this way because of hardware limitations on the machine this program was initially developed on but for this Colab implementation it would make more sense to put all available data into one large training set. \n",
        "\n",
        "The `x` and `y` arguments are provided as dictionaries since different models might use different inputs. This way the model will only use the variables it requests. \n",
        "\n",
        "The model is saved intermediatley if the `'save_periodically'` below Model Saving in `architecture` is set to `True` at intervals specified by `'save_interval'`.\n",
        "\n",
        "https://keras.io/api/models/model_training_apis/#fit-method "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIAwdPPSA4Tt",
        "colab_type": "text"
      },
      "source": [
        "## fit "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKFIKvF3AF3-",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def train(self, series_stacks):\n",
        "    tic = time.time()\n",
        "    self.history = [None]\n",
        "    self.loss = [None]\n",
        "    self.val_loss = [None]\n",
        "    keys = list(series_stacks.keys())\n",
        "    for h in range(len(keys)):\n",
        "        series_stack = series_stacks[keys[h]]\n",
        "        X, Y = data_splitter(self, series_stack, ['train', 'validation'])\n",
        "        if self.arch['noise'] == True:\n",
        "            X += (np.random.rand(np.shape(X)[0],np.shape(X)[1],np.shape(X)[2])-self.arch['noise_mean'])/self.arch['noise_var']\n",
        "        if np.shape(X)[0] == 0:\n",
        "            pass\n",
        "        else:         \n",
        "          history = self.model.fit(\n",
        "              x = X,#patterns,\n",
        "              y = Y,#targets, \n",
        "              batch_size = self.arch['batch_size'],\n",
        "              epochs = self.arch['epochs'], \n",
        "              verbose = 1, # Prints training progress\n",
        "              callbacks=self.early_stopping, #self.learning_rate_scheduler],\n",
        "              validation_split = self.arch['data_split']['validation']/100,\n",
        "              shuffle = self.arch['shuffle'])\n",
        "          self.history.append(history)\n",
        "          self.loss.extend(history.history['loss'])\n",
        "          self.val_loss.extend(history.history['val_loss'])  \n",
        "          if self.arch['save_periodically'] == True and i % self.arch['save_interval'] == 0:\n",
        "              save_model(self.model,self.arch)  \n",
        "    self.model.summary()\n",
        "    self.toc = np.round(time.time()-tic,1)\n",
        "    print('Elapsed time: ', self.toc)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRqRDFK3Jj4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_by_batch(self, series_stacks):\n",
        "    tic = time.time()\n",
        "    self.history = [None]\n",
        "    self.loss = [None]\n",
        "    self.val_loss = [None]\n",
        "    stacks_keys = list(series_stacks.keys()) # 100%, 90%, etc.\n",
        "    for h in range(len(stacks_keys)):\n",
        "        series_stack = series_stacks[stacks_keys[h]][self.arch['preprocess_type']]\n",
        "        batch_keys = list(series_stack.keys()) # batches\n",
        "        for j in range(len(batch_keys)):\n",
        "            series = series_stack[batch_keys[j]]\n",
        "            #print('\\nTraining on ', keys[h],'% healthy data.\\n')\n",
        "            #print('\\n Number of series being used for training:', int(len(series_stack[self.arch['preprocess_type']])*(self.arch['data_split']['train']+self.arch['data_split']['validation'])/100), '\\n')\n",
        "            X, Y = batch_splitter(self, series, ['train', 'validation'])\n",
        "            if self.arch['noise'] == True:\n",
        "                X += (np.random.rand(np.shape(X)[0],np.shape(X)[1])-self.arch['noise_mean'])/self.arch['noise_var']\n",
        "            if np.shape(X)[0] == 0:\n",
        "                pass\n",
        "            else:         \n",
        "                history = self.model.fit(\n",
        "                    x = X,#patterns,\n",
        "                    y = Y,#targets, \n",
        "                    batch_size = self.arch['batch_size'],\n",
        "                    epochs=self.arch['epochs'], \n",
        "                    verbose=2,\n",
        "                    callbacks=self.early_stopping, #self.learning_rate_scheduler],\n",
        "                    validation_split = self.arch['data_split']['validation']/100,\n",
        "                    shuffle = self.arch['shuffle'])\n",
        "                self.history.append(history)\n",
        "                self.loss.extend(history.history['loss'])\n",
        "                self.val_loss.extend(history.history['val_loss'])  \n",
        "                if self.arch['save_periodically'] == True and i % self.arch['save_interval'] == 0:\n",
        "                    save_model(self.model,self.arch)  \n",
        "    self.model.summary()\n",
        "    self.toc = np.round(time.time()-tic,1)\n",
        "    print('Elapsed time: ', self.toc)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgMhJQzOaZs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_generator(self, series_stacks):\n",
        "    tic = time.time()\n",
        "    self.history = [None]\n",
        "    self.loss = [None]\n",
        "    self.val_loss = [None]\n",
        "    keys = list(series_stacks.keys())\n",
        "    series_stack = series_stacks['100%'][self.arch['preprocess_type']]\n",
        "    history = self.model.fit(\n",
        "        x = generator(\n",
        "            self,\n",
        "            series_stack,\n",
        "            ['train']\n",
        "        ),\n",
        "        epochs = self.arch['epochs'], \n",
        "        verbose = 1,\n",
        "        callbacks = self.early_stopping,\n",
        "        validation_data = generator(\n",
        "            self,\n",
        "            series_stack,\n",
        "            ['validation']\n",
        "        ),\n",
        "        steps_per_epoch = self.arch['steps_per_epoch'],\n",
        "        validation_steps = self.arch['validation_steps'])\n",
        "    self.history.append(history)\n",
        "    self.loss.extend(history.history['loss'])\n",
        "    self.val_loss.extend(history.history['val_loss'])  \n",
        "    if self.arch['save_periodically'] == True and i % self.arch['save_interval'] == 0:\n",
        "        save_model(self.model,self.arch)  \n",
        "    self.model.summary()\n",
        "    self.toc = np.round(time.time()-tic,1)\n",
        "    print('Elapsed time: ', self.toc)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkCAQeUlyyy1",
        "colab_type": "text"
      },
      "source": [
        "The chosen function is defined as the class method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhs4lWPXOxz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train_process == 'entire_set':\n",
        "    NeuralNet.train = train\n",
        "elif train_process == 'batch_by_batch':\n",
        "    NeuralNet.train = batch_by_batch\n",
        "elif train_process == 'generator':\n",
        "    NeuralNet.train = train_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1htE2_3FGoW",
        "colab_type": "text"
      },
      "source": [
        "## RMSE function\n",
        "The Tensorflow and NumPy respective implementations of the Root mean square error function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENWH2mneFGvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(true, prediction):\n",
        "    return backend.sqrt(backend.mean(backend.square(prediction - true), axis=-1))\n",
        "def rmse_np(true, prediction):\n",
        "    return np.sqrt(np.mean(np.square(prediction - true), axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbgZDf8aHQ1P",
        "colab_type": "text"
      },
      "source": [
        "## Data splitting\n",
        "The generator function takes a series via the `batch` argument and reshapes it into the form that fits the model. The form is specified via the `'n_pattern_steps'`, `'n_target_steps'` and `'pattern_delta'` arguments in `architecture`. \n",
        "\n",
        "Here is an example of what `generator` returns if `'n_pattern_steps'` is 4, `'n_target_steps'` is 2 and `'pattern_delta'` is 3. (Given that the total number of acceleration signals $m$ in the series happens to line up with the configuration. If it doesn't the last few acceleration signals are omitted.)\n",
        "\n",
        "$series = (a_{1}, a_{2}, ..., a_{m})$\n",
        "\n",
        "$pattern_1 = (a_{1}, a_{2}, a_{3}), target_1 = (a_{4}, a_{5})$\n",
        "\n",
        "$pattern_2 = (a_{4}, a_{5}, a_{6}), target_2 = (a_{7}, a_{8})$\n",
        "\n",
        "$pattern_n = (a_{m-4}, a_{m-3}, a_{m-2}), target_n = (a_{m-1}, a_{m})$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ2LTLR7HQ6d",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def data_splitter(self, stack, categories):\n",
        "    X = np.empty([0, self.arch['n_pattern_steps'], self.arch['features']])\n",
        "    Y = np.empty([0, self.arch['n_target_steps']])\n",
        "    for i in range(len(stack[self.arch['preprocess_type']])):\n",
        "      series = stack[self.arch['preprocess_type']]['batch'+str(i)]\n",
        "      if series.category in categories:\n",
        "          steps = get_steps(self, series)\n",
        "          x = np.empty([steps,self.arch['n_pattern_steps'], self.arch['features']])\n",
        "          y = np.empty([steps,self.arch['n_target_steps']])\n",
        "          for j in range(len(self.arch['pattern_sensors'])):    \n",
        "              for k in range(steps):    \n",
        "                  pattern_start = k*self.arch['pattern_delta']\n",
        "                  pattern_finish = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps']\n",
        "                  target_start = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps'] # +1?\n",
        "                  target_finish = k*self.arch['pattern_delta']+self.arch['delta']*(self.arch['n_pattern_steps']+self.arch['n_target_steps'])\n",
        "                  x[k,:,j] = series.data[j][pattern_start:pattern_finish]\n",
        "                  if self.arch['pattern_sensors'][j] == self.arch['target_sensor']:\n",
        "                      y[k,:] = series.data[j][target_start:target_finish]\n",
        "                  if self.arch['positions'] == True:\n",
        "                      x[k,:,-1] = np.arange(pattern_start, pattern_finish, self.arch['delta'])/biggest\n",
        "          X = np.append(X,x,axis=0)\n",
        "          Y = np.append(Y,y,axis=0)\n",
        "    return X, Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5F8fn1gMzDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_splitter(self, series, categories):\n",
        "    X = np.empty([0, self.arch['n_pattern_steps'], self.arch['features']])\n",
        "    Y = np.empty([0, self.arch['n_target_steps']])\n",
        "    if series.category in categories:\n",
        "        steps = get_steps(self, series)\n",
        "        x = np.empty([steps,self.arch['n_pattern_steps'], self.arch['features']])\n",
        "        y = np.empty([steps,self.arch['n_target_steps']])\n",
        "        for j in range(len(self.arch['pattern_sensors'])):    \n",
        "            for k in range(steps):    \n",
        "                pattern_start = k*self.arch['pattern_delta']\n",
        "                pattern_finish = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps']\n",
        "                target_start = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps'] # +1?\n",
        "                target_finish = k*self.arch['pattern_delta']+self.arch['delta']*(self.arch['n_pattern_steps']+self.arch['n_target_steps'])\n",
        "                x[k,:,j] = series.data[j][pattern_start:pattern_finish]\n",
        "                if self.arch['pattern_sensors'][j] == self.arch['target_sensor']:\n",
        "                    y[k,:] = series.data[j][target_start:target_finish]\n",
        "                if self.arch['positions'] == True:\n",
        "                    x[k,:,-1] = np.arange(pattern_start, pattern_finish, self.arch['delta'])/biggest\n",
        "        X = np.append(X,x,axis=0)\n",
        "        Y = np.append(Y,y,axis=0)\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3lqKKCla5-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(self,stack,categories):\n",
        "    i = 0\n",
        "    while True:\n",
        "        X = np.empty([0, self.arch['n_pattern_steps'], self.arch['features']])\n",
        "        Y = np.empty([0, self.arch['n_target_steps']])\n",
        "        series = stack['batch'+str(i%len(stack))]\n",
        "        steps = get_steps(self, series)\n",
        "        if series.category in categories:\n",
        "            steps = get_steps(self, series)\n",
        "            x = np.empty([steps,self.arch['n_pattern_steps'], self.arch['features']])\n",
        "            y = np.empty([steps,self.arch['n_target_steps']])\n",
        "            for j in range(len(self.arch['pattern_sensors'])):    \n",
        "                for k in range(steps):    \n",
        "                    pattern_start = k*self.arch['pattern_delta']\n",
        "                    pattern_finish = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps']\n",
        "                    target_start = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps'] # +1?\n",
        "                    target_finish = k*self.arch['pattern_delta']+self.arch['delta']*(self.arch['n_pattern_steps']+self.arch['n_target_steps'])\n",
        "                    x[k,:,j] = series.data[j][pattern_start:pattern_finish]\n",
        "                    if self.arch['pattern_sensors'][j] == self.arch['target_sensor']:\n",
        "                        y[k,:] = series.data[j][target_start:target_finish]\n",
        "                    if self.arch['positions'] == True:\n",
        "                        x[k,:,-1] = np.arange(pattern_start, pattern_finish, self.arch['delta'])/biggest\n",
        "            X = np.append(X,x,axis=0)\n",
        "            Y = np.append(Y,y,axis=0)\n",
        "            print(np.shape(X), np.shape(Y))\n",
        "            yield (X,Y)\n",
        "        i += 1             \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX9IWTuQPQc_",
        "colab_type": "text"
      },
      "source": [
        "# Some utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GygpFqr3MuLY",
        "colab_type": "text"
      },
      "source": [
        "`get_steps` calculates the number $n$ in the example above, i.e. the number of targets and patterns that fit into a series.l"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bri10edqDpfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_steps(self, series):\n",
        "    steps = int(\n",
        "        np.floor(\n",
        "            (series.n_steps-(self.arch['n_pattern_steps']+self.arch['n_target_steps']))/self.arch['pattern_delta']\n",
        "        )\n",
        "    )\n",
        "    return steps  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE4AILSHNDca",
        "colab_type": "text"
      },
      "source": [
        "Utility function for saving the model. The save location is specified in `'model_save_path'` in `architecture`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pjSYNBjNDjp",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "from keras.backend import manual_variable_initialization \n",
        "manual_variable_initialization(False)\n",
        "def save_model(model, arch):\n",
        "    print(arch)\n",
        "    model_json = model.to_json()\n",
        "    with open(arch['model_path']+arch['fname']+'.json', 'w') as json_file:\n",
        "        json_file.write(model_json)\n",
        "        # serialize weights to HDF5\n",
        "    model.save_weights(arch['model_path'] + arch['fname'] + '.h5')\n",
        "    print('Saved model:', arch['fname'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dz_l2BEOKGy",
        "colab_type": "text"
      },
      "source": [
        "Function for plotting the evolution of the train and validation losses during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn6t0iddFJlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(self):\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(self.loss)), self.loss, 'bo', label='Training loss', linewidth=0.3)\n",
        "    plt.plot(range(len(self.val_loss)), self.val_loss, 'ro', label='Validation loss', linewidth=0.3)\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss - RMSE')\n",
        "    plt.legend()\n",
        "    plt.savefig(fname = self.arch['loss_path'] + self.arch['name'] + '_loss_plot.png')\n",
        "    plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOKf4-vY6P0S",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCFlZXtLNc4N",
        "colab_type": "text"
      },
      "source": [
        "##Single layer LSTM CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QZSYNeDDpf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model6(arch): # Vanilla \n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            1),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            1),\n",
        "        #activation = 'tanh',\n",
        "        #recurrent_activation = 'sigmoid',\n",
        "        #use_bias = True,\n",
        "        #dropout = 0.1,\n",
        "        #recurrent_dropout = 0,\n",
        "        #unroll = False,\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_1)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R6isOG8Nd2T",
        "colab_type": "text"
      },
      "source": [
        "## Single layer LSTM non-CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyOn0icC-YgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model1(arch): # Vanilla \n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            1),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = LSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            1),\n",
        "        #activation = 'tanh',\n",
        "        #recurrent_activation = 'sigmoid',\n",
        "        #use_bias = True,\n",
        "        #dropout = 0.1,\n",
        "        #recurrent_dropout = 0,\n",
        "        #unroll = False,\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_1)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4daDwnsqNep5",
        "colab_type": "text"
      },
      "source": [
        "## Two-layer LSTM CPU with positions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHoJqUzVDpf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model3(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape = (\n",
        "            arch['n_pattern_steps'], \n",
        "            arch['features']),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        #activation = 'tanh',\n",
        "        #recurrent_activation = 'sigmoid',\n",
        "        #use_bias = True,\n",
        "        #dropout = 0.1,\n",
        "        #recurrent_dropout = 0,\n",
        "        #unroll = False,\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    hidden_lstm_2 = CuDNNLSTM(\n",
        "        arch['n_units']['second'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        #activation = 'tanh',\n",
        "        #recurrent_activation = 'sigmoid',\n",
        "        #use_bias = True,\n",
        "        #dropout = 0.1,\n",
        "        #recurrent_dropout = 0,\n",
        "        #unroll = False,\n",
        "        stateful = False)(hidden_lstm_1)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_2)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoFffoYslcIt",
        "colab_type": "text"
      },
      "source": [
        "## Two-layer LSTM CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyAn2AfDlcaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model7(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            1),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            1),\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    hidden_lstm_2 = CuDNNLSTM(\n",
        "        arch['n_units']['second'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            1),\n",
        "        stateful = False)(hidden_lstm_1)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_2)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tECYA-g02SI0",
        "colab_type": "text"
      },
      "source": [
        "## Single-layer LSTM CPU arbitrary sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DBr6Xqn2SSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model8(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            arch['features']),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = False,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_1)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDbZExFHvavt",
        "colab_type": "text"
      },
      "source": [
        "## Two-layer LSTM CPU arbitrary sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M56c5Ka2va6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model4(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            arch['features']),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    hidden_lstm_2 = CuDNNLSTM(\n",
        "        arch['n_units']['second'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        stateful = False)(hidden_lstm_1)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_2)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mm9vN52JHXd0",
        "colab_type": "text"
      },
      "source": [
        "## Three-layer LSTM CPU arbitrary sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojgRy7T4HXrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model9(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            arch['features']),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    hidden_lstm_2 = CuDNNLSTM(\n",
        "        arch['n_units']['second'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = True,\n",
        "        stateful = False)(hidden_lstm_1)\n",
        "\n",
        "    hidden_lstm_3 = CuDNNLSTM(\n",
        "        arch['n_units']['third'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        stateful = False)(hidden_lstm_2)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_3)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9pAF29YoQVE",
        "colab_type": "text"
      },
      "source": [
        "## Four-layer LSTM CPU arbitrary sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCUjGF-6oQfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model12(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            arch['features']),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    hidden_lstm_2 = CuDNNLSTM(\n",
        "        arch['n_units']['second'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = True,\n",
        "        stateful = False)(hidden_lstm_1)\n",
        "\n",
        "    hidden_lstm_3 = CuDNNLSTM(\n",
        "        arch['n_units']['third'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = True,\n",
        "        stateful = False)(hidden_lstm_2)\n",
        "\n",
        "    hidden_lstm_4 = CuDNNLSTM(\n",
        "        arch['n_units']['fourth'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = False,\n",
        "        stateful = False)(hidden_lstm_3)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_4)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjEV_2AdNfp5",
        "colab_type": "text"
      },
      "source": [
        "## Two-layer LSTM non-CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBE8byrH-xXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model2(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            1),\n",
        "        name = 'accel_input_90')\n",
        "\n",
        "    hidden_lstm_1 = LSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            1),\n",
        "        activation = 'tanh',\n",
        "        recurrent_activation = 'sigmoid',\n",
        "        use_bias = True,\n",
        "        dropout = 0.1,\n",
        "        recurrent_dropout = 0,\n",
        "        unroll = False,\n",
        "        return_sequences = True,\n",
        "        stateful = False)(accel_input)\n",
        "\n",
        "    hidden_lstm_2 = LSTM(\n",
        "        arch['n_units']['second'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            1),\n",
        "        activation = 'tanh',\n",
        "        recurrent_activation = 'sigmoid',\n",
        "        use_bias = True,\n",
        "        dropout = 0.1,\n",
        "        recurrent_dropout = 0,\n",
        "        unroll = False,\n",
        "        stateful = False)(hidden_lstm_1)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_2)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxKi6bi3XKDy",
        "colab_type": "text"
      },
      "source": [
        "## Single-layer MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp7vKW2lXKND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model10(arch):\n",
        "    \n",
        "    accel_input = Input(\n",
        "        shape = (\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        name = 'accel_input_'+arch['target_sensor'])\n",
        "    \n",
        "    flat = Flatten()(accel_input)\n",
        "\n",
        "    hidden_1 = Dense(\n",
        "        arch['n_units']['first'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'],\n",
        "        name = 'hidden_layer')(flat)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation = arch['Dense_activation'], \n",
        "        name='acceleration_output')(hidden_1) \n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixpuuephNiZ1",
        "colab_type": "text"
      },
      "source": [
        "## Two-layer MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8mO57bNjLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model5(arch):\n",
        "    \n",
        "    accel_input = Input(\n",
        "        shape = (\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        name = 'accel_input_'+arch['target_sensor'])\n",
        "    \n",
        "    flat = Flatten()(accel_input)\n",
        "\n",
        "    hidden_1 = Dense(\n",
        "        arch['n_units']['first'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'],\n",
        "        name = 'hidden_layer')(flat)\n",
        "\n",
        "    hidden_2 = Dense(\n",
        "        arch['n_units']['second'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'])(hidden_1)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation = 'tanh', \n",
        "        name = 'acceleration_output')(hidden_2)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xxvUIkyftGR",
        "colab_type": "text"
      },
      "source": [
        "## Three-layer MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9M7y63ftRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model13(arch):\n",
        "    \n",
        "    accel_input = Input(\n",
        "        shape = (\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        name = 'accel_input_'+arch['target_sensor'])\n",
        "    \n",
        "    flat = Flatten()(accel_input)\n",
        "\n",
        "    hidden_1 = Dense(\n",
        "        arch['n_units']['first'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'],\n",
        "        name = 'hidden_layer')(flat)\n",
        "\n",
        "    hidden_2 = Dense(\n",
        "        arch['n_units']['second'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'])(hidden_1)\n",
        "\n",
        "    hidden_3 = Dense(\n",
        "        arch['n_units']['third'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'])(hidden_2)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation = 'tanh', \n",
        "        name = 'acceleration_output')(hidden_3)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uRcYMh9oAkY",
        "colab_type": "text"
      },
      "source": [
        "## Four-layer MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdG4KDV5oArf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model11(arch):\n",
        "    \n",
        "    accel_input = Input(\n",
        "        shape = (\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        name = 'accel_input_'+arch['target_sensor'])\n",
        "    \n",
        "    flat = Flatten()(accel_input)\n",
        "\n",
        "    hidden_1 = Dense(\n",
        "        arch['n_units']['first'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'],\n",
        "        name = 'hidden_layer')(flat)\n",
        "\n",
        "    hidden_2 = Dense(\n",
        "        arch['n_units']['second'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'])(hidden_1)\n",
        "\n",
        "    hidden_3 = Dense(\n",
        "        arch['n_units']['third'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'])(hidden_2)\n",
        "\n",
        "    hidden_4 = Dense(\n",
        "        arch['n_units']['fourth'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'])(hidden_3)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation = 'tanh', \n",
        "        name = 'acceleration_output')(hidden_4)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4KP6X0ERR0F",
        "colab_type": "text"
      },
      "source": [
        "## Single-layer MLP dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzL0zm1ORScM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model14(arch):\n",
        "    \n",
        "    accel_input = Input(\n",
        "        shape = (\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        name = 'accel_input_'+arch['target_sensor'])\n",
        "    \n",
        "    dropout = Dropout(\n",
        "        arch['dropout_rate'],\n",
        "        name = 'dropout')(accel_input)\n",
        "    \n",
        "    flat = Flatten()(dropout)\n",
        "\n",
        "    hidden_1 = Dense(\n",
        "        arch['n_units']['first'],\n",
        "        activation = arch['Dense_activation'],\n",
        "        use_bias = arch['bias'],\n",
        "        name = 'hidden_layer')(flat)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation = arch['Dense_activation'], \n",
        "        name='acceleration_output')(hidden_1) \n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxzI6yxUSJ8V",
        "colab_type": "text"
      },
      "source": [
        "## Single-layer LSTM CPU arbitrary sensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DsV0qMTSKG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_up_model15(arch):\n",
        "\n",
        "    accel_input = Input(\n",
        "        shape=(\n",
        "            arch['n_pattern_steps'], \n",
        "            arch['features']),\n",
        "        name = 'accel_input_90')\n",
        "    \n",
        "    dropout = Dropout(\n",
        "        arch['dropout_rate'],\n",
        "        name = 'dropout')(accel_input)\n",
        "\n",
        "    hidden_lstm_1 = CuDNNLSTM(\n",
        "        arch['n_units']['first'],\n",
        "        batch_input_shape = (\n",
        "            arch['batch_size'],\n",
        "            arch['n_pattern_steps'],\n",
        "            arch['features']),\n",
        "        return_sequences = False,\n",
        "        stateful = False)(dropout)\n",
        "\n",
        "    output = Dense(\n",
        "        arch['n_target_steps'], \n",
        "        activation='tanh', \n",
        "        name='peak_output_90')(hidden_lstm_1)\n",
        "\n",
        "    model = Model(inputs = accel_input, outputs = output)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcJj1bUfvrbN",
        "colab_type": "text"
      },
      "source": [
        "# Initializing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXctT0E23Jn5",
        "colab_type": "text"
      },
      "source": [
        "Checking if there already is a trained model with the specified name. If so that model is loaded, if not a new one is initialized and trained. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjPlps7J3Jvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def initialize_model(arch):\n",
        "    try:\n",
        "        f = open(arch['model_path'] + arch['fname'] + '.json')\n",
        "        machine_stack.update({\n",
        "            arch['fname'] : NeuralNet(\n",
        "                  arch,\n",
        "                  existing_model = True)\n",
        "        })\n",
        "    except FileNotFoundError:\n",
        "        if arch['retrain'] == True:\n",
        "            machine_stack.update({\n",
        "                arch['fname_base'] : NeuralNet(\n",
        "                    arch,\n",
        "                    existing_model = True\n",
        "                    )\n",
        "                ,\n",
        "                arch['fname'] : NeuralNet(\n",
        "                    arch,\n",
        "                    existing_model = True\n",
        "                    )\n",
        "                })\n",
        "            NeuralNet.train(machine_stack[arch['fname']], series_stack) \n",
        "            save_model(machine_stack[arch['fname']].model, arch)\n",
        "            plot_loss(machine_stack[arch['fname']])\n",
        "        elif arch['retrain'] == False:                      \n",
        "            machine_stack.update({\n",
        "                arch['fname'] : NeuralNet(\n",
        "                    arch,\n",
        "                    existing_model = False)\n",
        "            })\n",
        "            NeuralNet.train(machine_stack[arch['fname']], series_stack) \n",
        "            save_model(machine_stack[arch['fname']].model, arch)\n",
        "            plot_loss(machine_stack[arch['fname']])\n",
        "    #return machine_stack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNmlVacXPg_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "machine_stack = {}\n",
        "if architecture_mode == 1:\n",
        "    initialize_model(architecture)\n",
        "elif architecture_mode == 2:\n",
        "    for i in range(len(arch_keys)):\n",
        "        initialize_model(architectures[arch_keys[i]])\n",
        "machine_stack\n",
        "machine_keys = list(machine_stack.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur6b4NBKlHdI",
        "colab_type": "text"
      },
      "source": [
        "## Loading a model doesn't work properly\n",
        "When a model is loaded it doesn't predict in the same way it does as when the model is created in the same run as it is tested. \n",
        "This is apparently an issue for others as well: https://github.com/keras-team/keras/issues/4875\n",
        "Thus far no working solution has been encountered, the workaraound would be to make sure the test results are obtained from the same run the model is trained in. This code below demonstrates that the weights are not the same when saved and loaded as they were initially. \n",
        "Likely causes:\n",
        "Either keras uses randomness when initializing a model or there is some information which is lost when the model is converted to .JSON and .h5 and back. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DF9GEWg5f-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m1 = machine_stack[machine_keys[0]].model\n",
        "m2 = load_model(machine_stack[machine_keys[0]].arch)\n",
        "print(np.array_equal(m1.weights, m2.weights))\n",
        "#print(m1.weights, m2.weights)\n",
        "'''\n",
        "for l1, l2 in zip(m1.layers, m2.layers):\n",
        "    print(l1.get_config() == l2.get_config())\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNPbkvwrzkQh",
        "colab_type": "text"
      },
      "source": [
        "# Plot a sample prediction\n",
        "Showing a sample prediction made by the net we just trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZVN0qQvzjbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_prediction(self, series_stack, s):\n",
        "    batch = 'batch2'\n",
        "    series = series_stack['100%'][self.arch['preprocess_type']][batch].data#[self.arch['target_sensor']]\n",
        "    start = s\n",
        "    end = start+self.arch['n_pattern_steps']\n",
        "    print(np.shape(series))\n",
        "    x = np.reshape(\n",
        "        np.array([np.array(series[:,start:end])]), \n",
        "        (1, self.arch['n_pattern_steps'], self.arch['features'])\n",
        "        )\n",
        "    prediction = self.model.predict(\n",
        "        x,\n",
        "        batch_size = 1,\n",
        "        verbose = 1)\n",
        "    ground_truth = series[0,end+1:end+1+self.arch['n_target_steps']]\n",
        "    err = rmse_np(ground_truth, prediction)\n",
        "    # Input\n",
        "    plt.plot(np.linspace(start,end, self.arch['n_pattern_steps']),series[0,start:end])\n",
        "    # Prediction\n",
        "    plt.plot(np.linspace(end+1,end+1+self.arch['n_target_steps'],self.arch['n_target_steps']), prediction[0], 'r*')\n",
        "    # Ground truth\n",
        "    plt.plot(np.linspace(end+1,end+1+self.arch['n_target_steps'],self.arch['n_target_steps']), ground_truth,'ko')\n",
        "    plt.legend(['Input', 'Prediction', 'Ground truth'])\n",
        "    plt.title(self.arch['model']+', '+self.arch['normalization'])\n",
        "    plt.xlabel('Timestep')\n",
        "    plt.ylabel('Normalized acceleration')\n",
        "    plt.text(\n",
        "        x = start, \n",
        "        y = 0, \n",
        "        s = 'RMSE = ' + str(np.round(a = err[0], decimals = 4))\n",
        "        )\n",
        "    if save_plots == True:\n",
        "        plt.savefig(self.arch['prediction_path']+self.arch['fname']+'_'+str(start)+'_'+batch+'_dataset'+str(self.arch['dataset'])+'.png')\n",
        "    plt.show()\n",
        "    return\n",
        "NeuralNet.sample_prediction = sample_prediction\n",
        "def generate_samples():\n",
        "    for j in [0,500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9500,10000,10500,11000,11500,12000,12500,13000,13500,14000,14500,15000,15500]: # These can be altered\n",
        "        for i in range(len(machine_keys)):\n",
        "            NeuralNet.sample_prediction(machine_stack[machine_keys[i]], series_stack, j)\n",
        "            pass\n",
        "generate_samples()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwgpV9UaFik7",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "Method for evaluating a model. It requires the same arguments as the `train ` method. The returned `results` contains lists that are used to plot the results. The model score is defined as the Root mean square error for the model on a single series, and should in general be the same as the models' loss function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVXIN4L7iCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_generator(self, series): Generates the samples for both methods\n",
        "    steps = get_steps(self, series)\n",
        "    #print(steps)\n",
        "    X = np.empty([steps,self.arch['n_pattern_steps'], self.arch['features']])\n",
        "    Y = np.empty([steps,self.arch['n_target_steps']])\n",
        "    for j in range(len(self.arch['pattern_sensors'])):     \n",
        "        for k in range(steps):    \n",
        "            pattern_start = k*self.arch['pattern_delta']\n",
        "            pattern_finish = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps']\n",
        "            target_start = k*self.arch['pattern_delta']+self.arch['delta']*self.arch['n_pattern_steps'] # +1?\n",
        "            target_finish = k*self.arch['pattern_delta']+self.arch['delta']*(self.arch['n_pattern_steps']+self.arch['n_target_steps'])\n",
        "            X[k,:,j] = series.data_dict[self.arch['normalization']][j][pattern_start:pattern_finish]\n",
        "            Y[k,:] = series.data_dict[self.arch['normalization']][j][target_start:target_finish]\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptjrou6EMYf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_evaluation(self,stacks): # method 2\n",
        "    stack = stacks[self.arch['preprocess_type']]\n",
        "    scores = []\n",
        "    speeds = []\n",
        "    damage_states = []\n",
        "    for i in range(len(stack)):\n",
        "        key = 'batch'+str(i%len(stack))\n",
        "        if stack[key].category == 'test':\n",
        "            series = stack[key]\n",
        "            patterns, targets = eval_generator(self, series)\n",
        "            speed = stack[key].speed\n",
        "            score = self.model.test_on_batch(\n",
        "              patterns, \n",
        "              targets, \n",
        "              reset_metrics = False,\n",
        "              return_dict = True\n",
        "              )\n",
        "            scores.extend([score[self.arch['metric']]])\n",
        "            speeds.extend([speed])\n",
        "            damage_states.extend([stack[key].damage_state])\n",
        "    results = {\n",
        "        'scores' : scores,\n",
        "        'speeds' : speeds,\n",
        "        'steps' : len(speeds[:]),\n",
        "        'damage_state' : damage_states[:]\n",
        "    }\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stM-ADyuFiqv",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(self, series_stack): # method 1\n",
        "    scores = []\n",
        "    speeds = []\n",
        "    damage_states = []\n",
        "    for i in range(len(series_stack[self.arch['preprocess_type']])):\n",
        "        series = series_stack[self.arch['preprocess_type']]['batch'+str(i)]\n",
        "        if series.category == 'test':\n",
        "            X, Y = eval_generator(self, series)\n",
        "            score = self.model.evaluate(\n",
        "                x = X,\n",
        "                y = Y,\n",
        "                batch_size = self.arch['eval_batch_size'],\n",
        "                verbose = 1,\n",
        "                return_dict = True)\n",
        "            speeds.extend([series.speed])\n",
        "            scores.extend([score[self.arch['metric']]])\n",
        "            damage_states.extend([series.damage_state])\n",
        "        \n",
        "    results = {\n",
        "        'scores' : scores[:],\n",
        "        'speeds' : speeds[:],\n",
        "        'steps' : len(speeds[:]),\n",
        "        'damage_state' : damage_states[:]\n",
        "    }\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO8xrT_q3hiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if eval_mode == 1:\n",
        "    NeuralNet.evaluation = evaluation\n",
        "elif eval_mode == 2:\n",
        "    NeuralNet.evaluation = batch_evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stE9RQfI5dLG",
        "colab_type": "text"
      },
      "source": [
        "# Performance plot\n",
        "This function plots RMSE (y-axis) versus speed (x-axis), and colors the datapoints according to their damage state. This makes it easy to see if the model performs better on healthy or unhealthy data, or if there is no difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0r8igkc5dP1",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "def plot_performance(score_stack, a, pod): # pod = prediction or forecast\n",
        "    cmap = plt.cm.rainbow_r\n",
        "    percentage_keys = list(score_stack)\n",
        "    percentage_nums = [int(percentage_keys[i][0:-1]) for i in range(len(percentage_keys))]\n",
        "    norm = colors.Normalize(vmin = min(percentage_nums), vmax = 100)\n",
        "    \n",
        "    for i in range(len(percentage_keys)): # Iterates over percentages\n",
        "        for j in range(len(score_stack[percentage_keys[i]]['speeds'])):\n",
        "            plt.plot(\n",
        "                score_stack[percentage_keys[i]]['speeds'][j], \n",
        "                score_stack[percentage_keys[i]]['scores'][j], \n",
        "                color = cmap(norm(score_stack[percentage_keys[i]]['damage_state'][j])), \n",
        "                marker = 'X' if score_stack[percentage_keys[i]]['damage_state'][j] == 100 else 'o'\n",
        "                )\n",
        "    plt.xlabel('Speed [km/h]')\n",
        "    plt.ylabel('Root Mean Square Error')\n",
        "    plt.title('Sample scores for '+ a['model']+', '+a['normalization'], pad = 25.0)\n",
        "    sm = plt.cm.ScalarMappable(cmap = cmap, norm = norm)\n",
        "    cbar = plt.colorbar(sm)\n",
        "    cbar.set_label('Young\\'s modulus percentage', rotation=270)\n",
        "    if save_plots == True:\n",
        "        plt.savefig(fname = a['plot_path']+a['name']+'_performance_plot_dataset'+str(a['dataset'])+'batch_evaluation'+'.png')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihQWKRY94Zm9",
        "colab_type": "text"
      },
      "source": [
        "This piece of code creates a dictionary with the RMSE scores for all available damage states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74KVtK5s4ZyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "keys = list(series_stack) # Get keys for series_stack in a list\n",
        "keys.sort()\n",
        "score_stacks = {}\n",
        "for i in range(len(arch_keys)):\n",
        "    score_stack ={}\n",
        "    for j in range(len(keys)):\n",
        "        score_stack.update({\n",
        "            keys[j] : NeuralNet.evaluation(machine_stack[machine_keys[i]], series_stack[keys[j]])\n",
        "        })\n",
        "    score_stacks.update({\n",
        "        i : score_stack\n",
        "    })\n",
        "\n",
        "    if architecture_mode == 2:\n",
        "        plot_performance(score_stack, architectures[arch_keys[i]], 'prediction')\n",
        "    elif architecture_mode == 1:\n",
        "        plot_performance(score_stack, architecture, 'prediction')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97m3A5TcJOEK",
        "colab_type": "text"
      },
      "source": [
        "# Confusion Matrix\n",
        "The function `get_binary_prediction` creates a confusion matrix. Firstly a distribution for the RMSE-values on healthy data is created, then the test data is compared to this distribution. If the probability of a series RMSE score is greater than 90 percent it is predicted to be healthy. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si6yeNZ1JOKk",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import scipy.stats as stats\n",
        "from scipy.stats import norm\n",
        "import numpy.polynomial.polynomial as poly\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "def get_binary_prediction(score_stack, arch):\n",
        "    damage_cases = list(score_stack.keys())\n",
        "    phi = np.empty(1)\n",
        "    p = np.empty(1)\n",
        "    labels = np.empty(1)\n",
        "    unhealthy = np.empty(1)\n",
        "    speeds = score_stack['100%']['speeds']\n",
        "    healthy = score_stack['100%']['scores']\n",
        "    indices = np.array(random.sample(range(len(healthy)), arch['fitting_points']))\n",
        "    coefs = poly.polyfit(\n",
        "        x = [speeds[i] for i in indices], \n",
        "        y = [healthy[i] for i in indices], \n",
        "        deg = arch['poly_deg'])\n",
        "    ffit = poly.polyval(speeds, coefs)\n",
        "    mu = np.mean(healthy)\n",
        "    var = np.var(healthy)\n",
        "    sigma = np.sqrt(var)\n",
        "    #mu, std = stats.norm.fit(healthy)\n",
        "    \n",
        "    plot_positive_range(arch, speeds, healthy, coefs, mu, sigma, damage_cases, score_stack, indices)\n",
        "    for i in range(len(damage_cases)):\n",
        "        p = np.append(p,np.absolute(score_stack[damage_cases[i]]['scores'] - poly.polyval(score_stack[damage_cases[i]]['speeds'], coefs)), axis=0)\n",
        "        phi = np.append(\n",
        "            phi,\n",
        "            stats.norm.cdf(np.absolute((score_stack[damage_cases[i]]['scores']-mu)/sigma)),\n",
        "            axis = 0\n",
        "            )\n",
        "        if damage_cases[i] == '100%':\n",
        "            labels = np.append(labels, np.zeros(len(score_stack[damage_cases[i]]['scores'])),axis=0)    \n",
        "        else: \n",
        "            labels = np.append(labels, np.ones(len(score_stack[damage_cases[i]]['scores'])),axis=0)\n",
        "            unhealthy = np.append(unhealthy, score_stack[damage_cases[i]]['scores'])\n",
        "    plt.figure(\n",
        "        num = None, \n",
        "        figsize=(8, 8), \n",
        "        dpi = 80, \n",
        "        facecolor = 'w', \n",
        "        edgecolor = 'k'\n",
        "        )\n",
        "    plot_pdf(healthy, bins = 8, color = 'g')\n",
        "    plot_pdf(unhealthy, bins = 4, color = 'r')\n",
        "    plt.show()\n",
        "    poly_prediction = np.heaviside(p-arch['z-score']*sigma, 1)\n",
        "    cdf_prediction = np.heaviside(phi - arch['probability_limit'], 1)\n",
        "    prediction_dict = {\n",
        "        'Phi' : phi[1:],\n",
        "        'poly_prediction' : poly_prediction[1:],\n",
        "        'cdf_prediction' : cdf_prediction[1:],\n",
        "        'labels' : labels[1:]}\n",
        "    prediction_dict.update({\n",
        "        'poly_confusion_matrix' : confusion_matrix(prediction_dict['labels'],prediction_dict['poly_prediction']),\n",
        "        'cdf_confusion_matrix' : confusion_matrix(prediction_dict['labels'],prediction_dict['cdf_prediction'])\n",
        "        })\n",
        "    return prediction_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuvYfkbvvYml",
        "colab_type": "text"
      },
      "source": [
        "## Histogram plot\n",
        "Plots a histogram or several with the distribution of sample scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9ilE1bM_WKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_pdf(data, bins, color): \n",
        "    mu, std = norm.fit(data)\n",
        "    plt.hist(data, bins = bins, density = True, alpha=0.6, color = color)\n",
        "    xmin, xmax = plt.xlim()\n",
        "    x = np.linspace(xmin, xmax, 100)\n",
        "    p = norm.pdf(x, mu, std)\n",
        "    plt.plot(x, p, 'k', linewidth=2)\n",
        "    title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
        "    plt.title(title)\n",
        "    #plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX3V1of4vlFl",
        "colab_type": "text"
      },
      "source": [
        "## Decision boundary plot\n",
        "Plots all samples and the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CprJY-5xfgdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.lines import Line2D\n",
        "def plot_positive_range(arch, speeds, healthy, coefs, mu, sigma, damage_cases, score_stack, indices):\n",
        "    plt.figure(\n",
        "        num = None, \n",
        "        figsize=(8, 8), \n",
        "        dpi = 80, \n",
        "        facecolor = 'w', \n",
        "        edgecolor = 'k'\n",
        "        )\n",
        "    plt.plot(speeds,healthy,'ro')\n",
        "    for i in range(len(damage_cases)): # Loops over damage cases\n",
        "        if damage_cases[i] == '100%': # Plots healthy samples not used to fit regression\n",
        "            plt.plot(np.delete(arr = score_stack[damage_cases[i]]['speeds'], obj = indices), np.delete(arr = score_stack[damage_cases[i]]['scores'], obj = indices), 'yo')\n",
        "        else: # Plots unhealty samples\n",
        "            plt.plot(score_stack[damage_cases[i]]['speeds'], score_stack[damage_cases[i]]['scores'], 'k+')\n",
        "    left, right = plt.xlim()\n",
        "    base_x = np.linspace(left, right) # X-axis\n",
        "    base_y = poly.polyval(base_x, coefs) # Polynomial approximation\n",
        "    upper = sigma * arch['z-score'] # Upper decision boundary\n",
        "    lower = -sigma * arch['z-score'] # Lower decision boundary\n",
        "    plt.plot(base_x, base_y,'b')\n",
        "    plt.plot(base_x, base_y + upper,'g')\n",
        "    plt.plot(base_x, base_y + lower ,'g')\n",
        "    plt.fill_between(base_x, base_y + lower, base_y + upper)\n",
        "    custom_legend = [Line2D([0], [0], color = 'y', marker = 'o', linestyle = None),\n",
        "                     Line2D([0], [0], color = 'r', marker = 'o', linestyle = None),\n",
        "                     Line2D([0], [0], color = 'k', marker = '+', linestyle = None),\n",
        "                     Line2D([0], [0], color = 'b'),\n",
        "                     Line2D([0], [0], color = 'g')]\n",
        "    plt.legend(custom_legend, ['Healthy samples', 'Healthy samples used to fit regression', 'Unhealthy samples', 'Regression line', 'Decision boundaries'])\n",
        "    plt.xlabel('Speed')\n",
        "    plt.ylabel('Errors')\n",
        "    plt.title('Decision boundaries and samples')\n",
        "    if save_plots == True:\n",
        "        plt.savefig(fname = arch['plot_path'] + arch['name'] + 'decision_boundary' + '.png')\n",
        "    plt.show()\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWQzYV01iRoy",
        "colab_type": "text"
      },
      "source": [
        "## Plotting the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8TqtonbJP5A",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sn\n",
        "def plot_confusion(prediction, a):\n",
        "    tempkeys = list(a['confusion_matrices'].keys())\n",
        "    for i in range(len(a['confusion_matrices'])): # Loops over all confusion matrices\n",
        "        data = np.array(prediction[tempkeys[i]+'_confusion_matrix'])\n",
        "        text = np.asarray(\n",
        "            [['True Negatives','False Positives'],\n",
        "            ['False Negatives','True Positives']]\n",
        "            )\n",
        "        labels = (np.asarray([\"{0}\\n{1:.0f}\".format(text,data) for text, data in zip(text.flatten(), data.flatten())])).reshape(2,2)\n",
        "        sn.heatmap(\n",
        "            prediction[tempkeys[i]+'_confusion_matrix'], \n",
        "            annot = labels,\n",
        "            fmt='', \n",
        "            cbar = False,\n",
        "            cmap = 'binary')\n",
        "        plt.title(a['model']+', '+a['normalization']+' '+a['confusion_matrices'][tempkeys[i]])\n",
        "        if save_plots == True:\n",
        "            plt.savefig(fname = a['plot_path']+a['name']+'_'+tempkeys[i]+'_confusion_matrix_.png')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOGLJDBHiaeX",
        "colab_type": "text"
      },
      "source": [
        "Choses the appropriate architecture mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlU4Bkx9Uxar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if architecture_mode == 2:\n",
        "    for i in range(len(arch_keys)): # Loops over all models \n",
        "        binary_prediction = get_binary_prediction(score_stacks[i],architectures[arch_keys[i]])\n",
        "        plot_confusion(binary_prediction,architectures[arch_keys[i]])\n",
        "elif architecture_mode == 1:\n",
        "        binary_forecast_prediction = get_binary_prediction(score_stack,architecture)\n",
        "        plot_confusion(binary_forecast_prediction,architecture)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}